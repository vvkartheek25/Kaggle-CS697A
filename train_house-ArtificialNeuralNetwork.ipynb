{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Alley'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])\n",
    "df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['GarageYrBlt'],axis=1,inplace=True)\n",
    "df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])\n",
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass       0\n",
       "MSZoning         0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "Street           0\n",
       "                ..\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SaleType         0\n",
       "SaleCondition    0\n",
       "SalePrice        0\n",
       "Length: 75, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Id'],axis=1,inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a887773d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a8872db80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... EnclosedPorch 3SsnPorch ScreenPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "1    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "2    AllPub    Inside       Gtl  ...             0         0           0   \n",
       "3    AllPub    Corner       Gtl  ...           272         0           0   \n",
       "4    AllPub       FR2       Gtl  ...             0         0           0   \n",
       "\n",
       "  PoolArea MiscVal  MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0        0       0       2    2008        WD         Normal    208500  \n",
       "1        0       0       5    2007        WD         Normal    181500  \n",
       "2        0       0       9    2008        WD         Normal    223500  \n",
       "3        0       0       2    2006        WD        Abnorml    140000  \n",
       "4        0       0      12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df=df.copy()\n",
    "#Combine Test Data \n",
    "test_df=pd.read_csv('formulatedtest.csv')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2881 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0             60         65.0     8450            7            5       2003   \n",
       "1             20         80.0     9600            6            8       1976   \n",
       "2             60         68.0    11250            7            5       2001   \n",
       "3             70         60.0     9550            7            5       1915   \n",
       "4             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1454         160         21.0     1936            4            7       1970   \n",
       "1455         160         21.0     1894            4            5       1970   \n",
       "1456          20        160.0    20000            5            7       1960   \n",
       "1457          85         62.0    10441            5            5       1992   \n",
       "1458          60         74.0     9627            7            5       1993   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0             2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1             1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2             2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3             1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4             2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "...            ...         ...         ...         ...  ...   ...   ...  ...   \n",
       "1454          1970         0.0         0.0         0.0  ...     0     0    1   \n",
       "1455          1970         0.0       252.0         0.0  ...     0     0    1   \n",
       "1456          1996         0.0      1224.0         0.0  ...     0     0    1   \n",
       "1457          1992         0.0       337.0         0.0  ...     0     0    1   \n",
       "1458          1994        94.0       758.0         0.0  ...     0     0    1   \n",
       "\n",
       "      Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0          1        0        0        0       0    1  0  \n",
       "1          1        0        0        0       0    1  0  \n",
       "2          1        0        0        0       0    1  0  \n",
       "3          0        0        0        0       1    0  0  \n",
       "4          1        0        0        0       0    1  0  \n",
       "...      ...      ...      ...      ...     ...  ... ..  \n",
       "1454       1        0        0        0       0    0  0  \n",
       "1455       0        0        0        1       0    0  0  \n",
       "1456       0        0        0        0       1    0  0  \n",
       "1457       1        0        0        0       0    0  0  \n",
       "1458       1        0        0        0       0    0  0  \n",
       "\n",
       "[2881 rows x 175 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]\n",
    "final_df.shape\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          2003       196.0       706.0         0.0  ...     0     0    1   \n",
       "1          1976         0.0       978.0         0.0  ...     0     0    1   \n",
       "2          2002       162.0       486.0         0.0  ...     0     0    1   \n",
       "3          1970         0.0       216.0         0.0  ...     0     0    1   \n",
       "4          2000       350.0       655.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    1  0  \n",
       "1       1        0        0        0       0    1  0  \n",
       "2       1        0        0        0       0    1  0  \n",
       "3       0        0        0        0       1    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]\n",
    "df_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          20         80.0    11622            5            6       1961   \n",
       "1          20         81.0    14267            6            6       1958   \n",
       "2          60         74.0    13830            5            5       1997   \n",
       "3          60         78.0     9978            6            6       1998   \n",
       "4         120         43.0     5005            8            5       1992   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  Min1  Min2  Typ  \\\n",
       "0          1961         0.0       468.0       144.0  ...     0     0    1   \n",
       "1          1958       108.0       923.0         0.0  ...     0     0    1   \n",
       "2          1998         0.0       791.0         0.0  ...     0     0    1   \n",
       "3          1998        20.0       602.0         0.0  ...     0     0    1   \n",
       "4          1992         0.0       263.0         0.0  ...     0     0    1   \n",
       "\n",
       "   Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0       1        0        0        0       0    0  0  \n",
       "1       1        0        0        0       0    0  0  \n",
       "2       1        0        0        0       0    0  0  \n",
       "3       1        0        0        0       0    0  0  \n",
       "4       1        0        0        0       0    1  0  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartheek\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)\n",
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading tensorflow-2.3.1-cp38-cp38-win_amd64.whl (342.5 MB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "\n",
      "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\kartheek\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.2-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\kartheek\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=4cb35b4f0e4af148ba312a314728800c8e595a9ee32b832c9a80f67f0300793f\n",
      "  Stored in directory: c:\\users\\kartheek\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: protobuf, absl-py, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, grpcio, tensorboard-plugin-wit, tensorboard, astunparse, termcolor, tensorflow-estimator, keras-preprocessing, google-pasta, opt-einsum, gast, tensorflow\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 keras-preprocessing-1.1.2 markdown-3.3.3 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from Keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from Keras) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from Keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\kartheek\\anaconda3\\lib\\site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\kartheek\\appdata\\roaming\\python\\python38\\site-packages (from h5py->Keras) (1.15.0)\n",
      "Installing collected packages: Keras\n",
      "Successfully installed Keras-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 181587.6094 - val_loss: 161038.0938\n",
      "Epoch 2/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 113538.3984 - val_loss: 73348.8906\n",
      "Epoch 3/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 67166.5625 - val_loss: 65576.6562\n",
      "Epoch 4/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 65519.0625 - val_loss: 63965.9648\n",
      "Epoch 5/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 61754.6328 - val_loss: 62958.1250\n",
      "Epoch 6/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 61344.6836 - val_loss: 61603.6406\n",
      "Epoch 7/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 58570.5625 - val_loss: 60491.3672\n",
      "Epoch 8/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 59155.1172 - val_loss: 59472.4492\n",
      "Epoch 9/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 56902.6094 - val_loss: 58478.4297\n",
      "Epoch 10/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 56204.1953 - val_loss: 57667.6172\n",
      "Epoch 11/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 54729.8203 - val_loss: 56715.5078\n",
      "Epoch 12/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 53522.6211 - val_loss: 55595.9258\n",
      "Epoch 13/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 51480.8750 - val_loss: 54685.6406\n",
      "Epoch 14/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 50566.8867 - val_loss: 53744.8281\n",
      "Epoch 15/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 49538.9727 - val_loss: 53389.7539\n",
      "Epoch 16/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 47545.5977 - val_loss: 52004.1758\n",
      "Epoch 17/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 47439.4688 - val_loss: 51095.7227\n",
      "Epoch 18/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 44725.0117 - val_loss: 50307.5664\n",
      "Epoch 19/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 44659.7617 - val_loss: 49476.9570\n",
      "Epoch 20/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 43711.3398 - val_loss: 48671.4648\n",
      "Epoch 21/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 42097.6211 - val_loss: 47849.9453\n",
      "Epoch 22/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 40743.9102 - val_loss: 47499.6992\n",
      "Epoch 23/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 40019.4883 - val_loss: 46378.0469\n",
      "Epoch 24/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 38891.5234 - val_loss: 46326.6953\n",
      "Epoch 25/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 38777.5664 - val_loss: 45301.6367\n",
      "Epoch 26/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 37753.3867 - val_loss: 44967.7148\n",
      "Epoch 27/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 37393.5977 - val_loss: 45988.5703\n",
      "Epoch 28/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 37222.0195 - val_loss: 44707.4219\n",
      "Epoch 29/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 37147.1953 - val_loss: 44662.3438\n",
      "Epoch 30/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 36464.4219 - val_loss: 44664.2578\n",
      "Epoch 31/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 36137.4961 - val_loss: 44596.3711\n",
      "Epoch 32/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36508.6523 - val_loss: 44963.0039\n",
      "Epoch 33/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36366.1016 - val_loss: 44700.1406\n",
      "Epoch 34/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 36453.0039 - val_loss: 44768.6406\n",
      "Epoch 35/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 36426.2266 - val_loss: 45744.6523\n",
      "Epoch 36/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36511.7695 - val_loss: 45073.3516\n",
      "Epoch 37/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35817.1562 - val_loss: 44745.7500\n",
      "Epoch 38/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 36466.6836 - val_loss: 44591.6016\n",
      "Epoch 39/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 36292.6016 - val_loss: 44959.9141\n",
      "Epoch 40/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35845.3398 - val_loss: 44707.9180\n",
      "Epoch 41/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35977.8906 - val_loss: 44795.5586\n",
      "Epoch 42/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36064.0742 - val_loss: 44691.9727\n",
      "Epoch 43/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35573.7539 - val_loss: 45025.3008\n",
      "Epoch 44/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 36214.2422 - val_loss: 44580.8750\n",
      "Epoch 45/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 36429.0703 - val_loss: 44556.1562\n",
      "Epoch 46/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36137.9297 - val_loss: 44666.3438\n",
      "Epoch 47/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35321.9805 - val_loss: 44474.3633\n",
      "Epoch 48/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36168.2109 - val_loss: 44466.9844\n",
      "Epoch 49/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 35810.7734 - val_loss: 44444.9961\n",
      "Epoch 50/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36146.9961 - val_loss: 44816.4648\n",
      "Epoch 51/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 36104.9258 - val_loss: 44727.0625\n",
      "Epoch 52/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35907.8789 - val_loss: 44695.0039\n",
      "Epoch 53/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35847.9336 - val_loss: 45380.1367\n",
      "Epoch 54/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35754.1992 - val_loss: 44539.6602\n",
      "Epoch 55/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 35651.1992 - val_loss: 44953.8789\n",
      "Epoch 56/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35979.8906 - val_loss: 44554.5000\n",
      "Epoch 57/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35844.4844 - val_loss: 44549.8438\n",
      "Epoch 58/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35824.9219 - val_loss: 44545.3359\n",
      "Epoch 59/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35716.3398 - val_loss: 44515.0664\n",
      "Epoch 60/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35560.5469 - val_loss: 44479.8867\n",
      "Epoch 61/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35578.9062 - val_loss: 44415.5898\n",
      "Epoch 62/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35651.7344 - val_loss: 44473.4570\n",
      "Epoch 63/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35129.6367 - val_loss: 44348.8711\n",
      "Epoch 64/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35694.8359 - val_loss: 45143.2188\n",
      "Epoch 65/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35862.0820 - val_loss: 44600.3281\n",
      "Epoch 66/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35585.1836 - val_loss: 44409.9609\n",
      "Epoch 67/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35910.7539 - val_loss: 44638.9570\n",
      "Epoch 68/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 36054.8008 - val_loss: 44539.4258\n",
      "Epoch 69/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35313.2031 - val_loss: 44400.6992\n",
      "Epoch 70/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35785.8164 - val_loss: 44442.3398\n",
      "Epoch 71/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35492.5781 - val_loss: 44396.0078\n",
      "Epoch 72/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 35591.9180 - val_loss: 44607.3594\n",
      "Epoch 73/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 35617.9844 - val_loss: 44447.2227\n",
      "Epoch 74/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 35533.8125 - val_loss: 44276.8711\n",
      "Epoch 75/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35377.1289 - val_loss: 44193.9219\n",
      "Epoch 76/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35239.9258 - val_loss: 44288.4922\n",
      "Epoch 77/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35563.4492 - val_loss: 44240.9062\n",
      "Epoch 78/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35577.4180 - val_loss: 44258.3789\n",
      "Epoch 79/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35073.4180 - val_loss: 45666.7266\n",
      "Epoch 80/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35359.9492 - val_loss: 44101.6562\n",
      "Epoch 81/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35318.9141 - val_loss: 44322.7695\n",
      "Epoch 82/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35438.5234 - val_loss: 44079.0781\n",
      "Epoch 83/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35037.8477 - val_loss: 44042.3203\n",
      "Epoch 84/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35298.3984 - val_loss: 45195.8828\n",
      "Epoch 85/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35258.2539 - val_loss: 44292.8438\n",
      "Epoch 86/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35093.6250 - val_loss: 44889.9492\n",
      "Epoch 87/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 35183.2383 - val_loss: 44211.2617\n",
      "Epoch 88/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 34903.7969 - val_loss: 43957.2148\n",
      "Epoch 89/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 35067.5234 - val_loss: 43858.1211\n",
      "Epoch 90/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 34547.6328 - val_loss: 43890.5039\n",
      "Epoch 91/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34529.2969 - val_loss: 44248.3984\n",
      "Epoch 92/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34886.1094 - val_loss: 43903.1406\n",
      "Epoch 93/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34713.6875 - val_loss: 43896.6641\n",
      "Epoch 94/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 34882.2344 - val_loss: 43852.1328\n",
      "Epoch 95/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 34281.5508 - val_loss: 44154.3125\n",
      "Epoch 96/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 34561.6445 - val_loss: 43953.8320\n",
      "Epoch 97/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34870.3828 - val_loss: 43880.7734\n",
      "Epoch 98/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 34794.6250 - val_loss: 43865.5352\n",
      "Epoch 99/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34946.4766 - val_loss: 44868.8125\n",
      "Epoch 100/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 34631.4609 - val_loss: 43756.3359\n",
      "Epoch 101/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34489.3359 - val_loss: 43783.0391\n",
      "Epoch 102/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34273.3398 - val_loss: 45277.4141\n",
      "Epoch 103/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33994.6641 - val_loss: 43872.2031\n",
      "Epoch 104/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34302.0430 - val_loss: 43721.6680\n",
      "Epoch 105/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 34175.5625 - val_loss: 43709.8359\n",
      "Epoch 106/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 34526.9102 - val_loss: 43772.7734\n",
      "Epoch 107/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 34350.5352 - val_loss: 44009.3477\n",
      "Epoch 108/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 34447.5039 - val_loss: 44242.9141\n",
      "Epoch 109/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 34384.1758 - val_loss: 43697.3789\n",
      "Epoch 110/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34360.0703 - val_loss: 44024.4297\n",
      "Epoch 111/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34332.4375 - val_loss: 43660.5078\n",
      "Epoch 112/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34427.9219 - val_loss: 43668.6523\n",
      "Epoch 113/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34358.4766 - val_loss: 44082.0664\n",
      "Epoch 114/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34089.4531 - val_loss: 43641.5703\n",
      "Epoch 115/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34106.6367 - val_loss: 44859.7773\n",
      "Epoch 116/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34274.2891 - val_loss: 44299.2969\n",
      "Epoch 117/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33807.6211 - val_loss: 43611.7305\n",
      "Epoch 118/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 34092.6992 - val_loss: 43944.0312\n",
      "Epoch 119/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 33893.4414 - val_loss: 43642.8008\n",
      "Epoch 120/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 33840.9961 - val_loss: 44070.4648\n",
      "Epoch 121/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 33769.0781 - val_loss: 43682.3906\n",
      "Epoch 122/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 33819.7109 - val_loss: 43859.8867\n",
      "Epoch 123/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 34007.6328 - val_loss: 43600.7422\n",
      "Epoch 124/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 33557.4023 - val_loss: 43562.4609\n",
      "Epoch 125/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 33480.4062 - val_loss: 43618.5664\n",
      "Epoch 126/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33579.9570 - val_loss: 43559.3125\n",
      "Epoch 127/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33580.2344 - val_loss: 43792.1562\n",
      "Epoch 128/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 33879.7930 - val_loss: 43504.5703\n",
      "Epoch 129/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 33434.5664 - val_loss: 44117.7305\n",
      "Epoch 130/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 33558.4727 - val_loss: 43975.8086\n",
      "Epoch 131/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33426.4922 - val_loss: 43705.3711\n",
      "Epoch 132/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33538.0781 - val_loss: 43712.0781\n",
      "Epoch 133/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33241.1289 - val_loss: 43512.1523\n",
      "Epoch 134/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33405.8711 - val_loss: 43566.3789\n",
      "Epoch 135/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33530.5312 - val_loss: 43656.6602\n",
      "Epoch 136/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32925.2734 - val_loss: 43650.6523\n",
      "Epoch 137/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33394.8633 - val_loss: 43373.0156\n",
      "Epoch 138/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33030.9688 - val_loss: 44059.1094\n",
      "Epoch 139/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33301.3555 - val_loss: 43479.7422\n",
      "Epoch 140/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33163.0391 - val_loss: 43989.0078\n",
      "Epoch 141/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33480.4258 - val_loss: 43850.8008\n",
      "Epoch 142/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33162.7305 - val_loss: 44264.8711\n",
      "Epoch 143/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33174.7617 - val_loss: 43462.1992\n",
      "Epoch 144/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33271.4844 - val_loss: 43688.4844\n",
      "Epoch 145/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33319.4727 - val_loss: 43422.7656\n",
      "Epoch 146/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33269.3008 - val_loss: 43565.3906\n",
      "Epoch 147/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33028.4414 - val_loss: 43431.4375\n",
      "Epoch 148/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33204.2227 - val_loss: 43607.0938\n",
      "Epoch 149/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32895.6172 - val_loss: 43738.1836\n",
      "Epoch 150/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32958.5469 - val_loss: 43686.7539\n",
      "Epoch 151/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33175.7695 - val_loss: 43782.9531\n",
      "Epoch 152/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33339.4297 - val_loss: 43630.9805\n",
      "Epoch 153/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32925.4688 - val_loss: 43470.1055\n",
      "Epoch 154/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32605.2656 - val_loss: 43366.4766\n",
      "Epoch 155/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32648.7598 - val_loss: 43826.6055\n",
      "Epoch 156/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 33048.2695 - val_loss: 43383.0391\n",
      "Epoch 157/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32413.8574 - val_loss: 43760.3008\n",
      "Epoch 158/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32711.4824 - val_loss: 44009.4141\n",
      "Epoch 159/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32096.0625 - val_loss: 43459.9141\n",
      "Epoch 160/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32327.8672 - val_loss: 44699.3281\n",
      "Epoch 161/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 32683.6719 - val_loss: 43586.2031\n",
      "Epoch 162/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 32374.0020 - val_loss: 43406.5273\n",
      "Epoch 163/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 32584.9258 - val_loss: 43415.4688\n",
      "Epoch 164/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 32279.3887 - val_loss: 43432.4141\n",
      "Epoch 165/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31830.3887 - val_loss: 43508.0273\n",
      "Epoch 166/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 32479.9824 - val_loss: 43418.6289\n",
      "Epoch 167/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 32671.1914 - val_loss: 43514.0156\n",
      "Epoch 168/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31914.8184 - val_loss: 43586.3359\n",
      "Epoch 169/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 32522.9453 - val_loss: 43560.4805\n",
      "Epoch 170/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 32492.9922 - val_loss: 43492.2188\n",
      "Epoch 171/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 32004.3906 - val_loss: 43465.8711\n",
      "Epoch 172/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32174.1875 - val_loss: 44222.2773\n",
      "Epoch 173/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 32202.8887 - val_loss: 43384.3633\n",
      "Epoch 174/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31937.6016 - val_loss: 43242.5000\n",
      "Epoch 175/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 32018.6074 - val_loss: 43372.7578\n",
      "Epoch 176/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31920.2559 - val_loss: 44644.2812\n",
      "Epoch 177/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 32575.3984 - val_loss: 43315.1914\n",
      "Epoch 178/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 32208.5059 - val_loss: 43295.9414\n",
      "Epoch 179/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31982.0586 - val_loss: 43942.3672\n",
      "Epoch 180/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31634.8242 - val_loss: 43304.5352\n",
      "Epoch 181/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31784.7559 - val_loss: 43366.4297\n",
      "Epoch 182/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31715.0391 - val_loss: 43675.1133\n",
      "Epoch 183/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31906.6914 - val_loss: 43701.3438\n",
      "Epoch 184/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31943.7812 - val_loss: 44093.9375\n",
      "Epoch 185/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31914.4492 - val_loss: 43607.5898\n",
      "Epoch 186/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31597.3262 - val_loss: 43281.2891\n",
      "Epoch 187/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31690.4277 - val_loss: 43218.8281\n",
      "Epoch 188/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31869.4570 - val_loss: 43441.2422\n",
      "Epoch 189/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31842.1680 - val_loss: 43552.9180\n",
      "Epoch 190/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31772.6445 - val_loss: 43526.7891\n",
      "Epoch 191/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 32034.4902 - val_loss: 44401.3984\n",
      "Epoch 192/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31816.6016 - val_loss: 44210.3828\n",
      "Epoch 193/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31726.9980 - val_loss: 43387.9141\n",
      "Epoch 194/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31531.6270 - val_loss: 43621.9531\n",
      "Epoch 195/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31430.2402 - val_loss: 43727.0195\n",
      "Epoch 196/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31649.4629 - val_loss: 43327.0117\n",
      "Epoch 197/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31488.2070 - val_loss: 43436.0938\n",
      "Epoch 198/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31292.4121 - val_loss: 43886.7578\n",
      "Epoch 199/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 31411.8633 - val_loss: 43386.5664\n",
      "Epoch 200/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31328.8691 - val_loss: 43660.0312\n",
      "Epoch 201/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31537.6641 - val_loss: 43408.0625\n",
      "Epoch 202/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31125.0488 - val_loss: 44349.8789\n",
      "Epoch 203/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31776.7949 - val_loss: 44256.2461\n",
      "Epoch 204/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31429.0625 - val_loss: 43321.1328\n",
      "Epoch 205/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31115.3633 - val_loss: 43360.7617\n",
      "Epoch 206/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31542.7832 - val_loss: 43538.5078\n",
      "Epoch 207/1000\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 31313.4453 - val_loss: 43611.6523\n",
      "Epoch 208/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31225.2480 - val_loss: 44233.2852\n",
      "Epoch 209/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31295.9902 - val_loss: 43560.8555\n",
      "Epoch 210/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 30831.2148 - val_loss: 43434.4688\n",
      "Epoch 211/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 30878.6250 - val_loss: 43539.0938\n",
      "Epoch 212/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 31125.5098 - val_loss: 44680.7969\n",
      "Epoch 213/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31327.7109 - val_loss: 43574.6016\n",
      "Epoch 214/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 31292.6719 - val_loss: 43435.7422\n",
      "Epoch 215/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31267.7754 - val_loss: 43772.9883\n",
      "Epoch 216/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30748.8047 - val_loss: 43482.1250\n",
      "Epoch 217/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 30965.8887 - val_loss: 43784.6484\n",
      "Epoch 218/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31066.0898 - val_loss: 43688.1875\n",
      "Epoch 219/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30572.0879 - val_loss: 43557.5352\n",
      "Epoch 220/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 31151.1738 - val_loss: 44137.7031\n",
      "Epoch 221/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30301.8125 - val_loss: 43626.2969\n",
      "Epoch 222/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30743.1953 - val_loss: 43595.6992\n",
      "Epoch 223/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30626.1914 - val_loss: 43706.3281\n",
      "Epoch 224/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 31256.8477 - val_loss: 43618.4258\n",
      "Epoch 225/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30872.2637 - val_loss: 43515.0820\n",
      "Epoch 226/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30946.5371 - val_loss: 43512.0820\n",
      "Epoch 227/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30501.3652 - val_loss: 43671.9453\n",
      "Epoch 228/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 31050.2227 - val_loss: 43573.0156\n",
      "Epoch 229/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30432.6094 - val_loss: 44632.5508\n",
      "Epoch 230/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30353.9766 - val_loss: 45306.9805\n",
      "Epoch 231/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 31154.4980 - val_loss: 43523.3359\n",
      "Epoch 232/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30176.4434 - val_loss: 43634.2188\n",
      "Epoch 233/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30954.7090 - val_loss: 43653.6758\n",
      "Epoch 234/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30407.9395 - val_loss: 43378.9609\n",
      "Epoch 235/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30817.6992 - val_loss: 43601.2578\n",
      "Epoch 236/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29914.7266 - val_loss: 43507.7500\n",
      "Epoch 237/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30663.9785 - val_loss: 44036.6289\n",
      "Epoch 238/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30290.8418 - val_loss: 43521.7344\n",
      "Epoch 239/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30292.3477 - val_loss: 44147.0000\n",
      "Epoch 240/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30317.2949 - val_loss: 44209.0234\n",
      "Epoch 241/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30521.5488 - val_loss: 43629.4102\n",
      "Epoch 242/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 30508.8613 - val_loss: 43485.0156\n",
      "Epoch 243/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30611.7070 - val_loss: 43564.8867\n",
      "Epoch 244/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30669.8652 - val_loss: 43877.0664\n",
      "Epoch 245/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30670.8594 - val_loss: 43597.2188\n",
      "Epoch 246/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30511.4609 - val_loss: 43494.8867\n",
      "Epoch 247/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30005.5977 - val_loss: 43796.7539\n",
      "Epoch 248/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30121.7559 - val_loss: 43634.7812\n",
      "Epoch 249/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29615.3496 - val_loss: 44253.6211\n",
      "Epoch 250/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30219.0078 - val_loss: 43983.0312\n",
      "Epoch 251/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30374.7930 - val_loss: 43524.1875\n",
      "Epoch 252/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30145.9844 - val_loss: 43616.0664\n",
      "Epoch 253/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29981.5938 - val_loss: 43603.4844\n",
      "Epoch 254/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30329.5938 - val_loss: 45505.4258\n",
      "Epoch 255/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29930.6992 - val_loss: 45041.4023\n",
      "Epoch 256/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30172.8262 - val_loss: 43593.5469\n",
      "Epoch 257/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 30424.4473 - val_loss: 44211.4258\n",
      "Epoch 258/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30383.9023 - val_loss: 44208.0234\n",
      "Epoch 259/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30055.6660 - val_loss: 43909.5508\n",
      "Epoch 260/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29988.7871 - val_loss: 43544.7812\n",
      "Epoch 261/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30236.7207 - val_loss: 43416.7148\n",
      "Epoch 262/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29891.2051 - val_loss: 43652.3906\n",
      "Epoch 263/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29934.7617 - val_loss: 43545.6094\n",
      "Epoch 264/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29868.2461 - val_loss: 43470.0703\n",
      "Epoch 265/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29928.4746 - val_loss: 43833.7773\n",
      "Epoch 266/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29595.6348 - val_loss: 44386.2305\n",
      "Epoch 267/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 30172.8164 - val_loss: 43318.1836\n",
      "Epoch 268/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29737.0723 - val_loss: 43325.7695\n",
      "Epoch 269/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29914.4980 - val_loss: 43535.2852\n",
      "Epoch 270/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29661.1328 - val_loss: 43514.1914\n",
      "Epoch 271/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29836.1328 - val_loss: 43595.5469\n",
      "Epoch 272/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29716.7344 - val_loss: 43961.2422\n",
      "Epoch 273/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29968.9258 - val_loss: 43185.9609\n",
      "Epoch 274/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29767.0352 - val_loss: 43509.0625\n",
      "Epoch 275/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29880.2012 - val_loss: 43529.6719\n",
      "Epoch 276/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29075.2949 - val_loss: 43260.0312\n",
      "Epoch 277/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29482.4863 - val_loss: 43211.4180\n",
      "Epoch 278/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 29808.9395 - val_loss: 43444.8086\n",
      "Epoch 279/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29306.0254 - val_loss: 43860.0586\n",
      "Epoch 280/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29645.1406 - val_loss: 43372.7422\n",
      "Epoch 281/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29373.7383 - val_loss: 43560.4805\n",
      "Epoch 282/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29336.9688 - val_loss: 43569.3164\n",
      "Epoch 283/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29288.9785 - val_loss: 43362.0547\n",
      "Epoch 284/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29158.9238 - val_loss: 43989.5039\n",
      "Epoch 285/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29303.7285 - val_loss: 43770.3555\n",
      "Epoch 286/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29235.0605 - val_loss: 43318.3633\n",
      "Epoch 287/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29072.0918 - val_loss: 43883.1719\n",
      "Epoch 288/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29264.9570 - val_loss: 43357.0977\n",
      "Epoch 289/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29348.9023 - val_loss: 43393.3555\n",
      "Epoch 290/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 29390.2031 - val_loss: 43628.8594\n",
      "Epoch 291/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29485.1621 - val_loss: 43329.5234\n",
      "Epoch 292/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28989.2207 - val_loss: 45293.9062\n",
      "Epoch 293/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29268.7383 - val_loss: 43182.8906\n",
      "Epoch 294/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29175.0586 - val_loss: 42883.2109\n",
      "Epoch 295/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29208.7012 - val_loss: 42991.9102\n",
      "Epoch 296/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29271.0000 - val_loss: 43059.3047\n",
      "Epoch 297/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29306.2246 - val_loss: 42907.0859\n",
      "Epoch 298/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29129.1758 - val_loss: 43153.8672\n",
      "Epoch 299/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29106.1211 - val_loss: 42841.0469\n",
      "Epoch 300/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28753.5586 - val_loss: 42975.4258\n",
      "Epoch 301/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29215.6406 - val_loss: 43219.1953\n",
      "Epoch 302/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29599.0293 - val_loss: 43478.5156\n",
      "Epoch 303/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29256.2129 - val_loss: 42791.7461\n",
      "Epoch 304/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 29081.0996 - val_loss: 43222.5469\n",
      "Epoch 305/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28528.2988 - val_loss: 43391.0859\n",
      "Epoch 306/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28995.8145 - val_loss: 45255.4531\n",
      "Epoch 307/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29315.1055 - val_loss: 43130.9141\n",
      "Epoch 308/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28862.6465 - val_loss: 43072.3594\n",
      "Epoch 309/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28937.1523 - val_loss: 43739.8281\n",
      "Epoch 310/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28914.4570 - val_loss: 42895.4805\n",
      "Epoch 311/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28545.0664 - val_loss: 42713.2578\n",
      "Epoch 312/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 28655.4883 - val_loss: 43335.3320\n",
      "Epoch 313/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 28971.1367 - val_loss: 43318.0977\n",
      "Epoch 314/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28813.9941 - val_loss: 43756.5625\n",
      "Epoch 315/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28654.7207 - val_loss: 42972.0859\n",
      "Epoch 316/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28697.9727 - val_loss: 44252.0859\n",
      "Epoch 317/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28557.7559 - val_loss: 42567.5703\n",
      "Epoch 318/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 28438.2480 - val_loss: 42745.0508\n",
      "Epoch 319/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 28660.7715 - val_loss: 43700.8633\n",
      "Epoch 320/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28600.5176 - val_loss: 43553.6328\n",
      "Epoch 321/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 28521.6855 - val_loss: 42987.5820\n",
      "Epoch 322/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 28241.7422 - val_loss: 42336.0234\n",
      "Epoch 323/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 28233.0176 - val_loss: 43799.7773\n",
      "Epoch 324/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 29291.9668 - val_loss: 42511.2656\n",
      "Epoch 325/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 28736.4180 - val_loss: 42896.3281\n",
      "Epoch 326/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28318.1582 - val_loss: 42204.0352\n",
      "Epoch 327/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28714.4434 - val_loss: 42473.3398\n",
      "Epoch 328/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28250.4121 - val_loss: 42804.7930\n",
      "Epoch 329/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28188.6250 - val_loss: 42370.1875\n",
      "Epoch 330/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28446.5039 - val_loss: 43132.6680\n",
      "Epoch 331/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27942.4062 - val_loss: 43263.4141\n",
      "Epoch 332/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28413.0684 - val_loss: 42631.3672\n",
      "Epoch 333/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 28245.1992 - val_loss: 42650.2461\n",
      "Epoch 334/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 28092.5371 - val_loss: 43446.2656\n",
      "Epoch 335/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 28086.7949 - val_loss: 42248.2852\n",
      "Epoch 336/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28151.8906 - val_loss: 42592.1211\n",
      "Epoch 337/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27978.6348 - val_loss: 45201.8008\n",
      "Epoch 338/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28351.5039 - val_loss: 42264.5469\n",
      "Epoch 339/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28468.1465 - val_loss: 42324.7891\n",
      "Epoch 340/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28082.5996 - val_loss: 43211.4297\n",
      "Epoch 341/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27303.3281 - val_loss: 42480.6758\n",
      "Epoch 342/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27981.4434 - val_loss: 42251.8906\n",
      "Epoch 343/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27567.1328 - val_loss: 42288.8672\n",
      "Epoch 344/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27955.1855 - val_loss: 42417.2617\n",
      "Epoch 345/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 28292.8945 - val_loss: 42106.0430\n",
      "Epoch 346/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27897.0391 - val_loss: 42655.4453\n",
      "Epoch 347/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27669.8418 - val_loss: 41938.0781\n",
      "Epoch 348/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 27509.3555 - val_loss: 42057.4961\n",
      "Epoch 349/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27827.0352 - val_loss: 41816.6250\n",
      "Epoch 350/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27605.0176 - val_loss: 42086.1914\n",
      "Epoch 351/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 27915.9570 - val_loss: 42053.0352\n",
      "Epoch 352/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27656.3281 - val_loss: 42841.6367\n",
      "Epoch 353/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27676.1797 - val_loss: 41952.3984\n",
      "Epoch 354/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27250.0840 - val_loss: 41938.6172\n",
      "Epoch 355/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 27789.3926 - val_loss: 43039.6016\n",
      "Epoch 356/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 27192.5664 - val_loss: 41907.3164\n",
      "Epoch 357/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27049.8691 - val_loss: 43165.8477\n",
      "Epoch 358/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27806.6289 - val_loss: 41907.6484\n",
      "Epoch 359/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27456.8926 - val_loss: 44048.7148\n",
      "Epoch 360/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27576.8164 - val_loss: 42596.2031\n",
      "Epoch 361/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27554.7031 - val_loss: 42155.4453\n",
      "Epoch 362/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27576.6992 - val_loss: 43387.0508\n",
      "Epoch 363/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27426.0684 - val_loss: 41692.3945\n",
      "Epoch 364/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27426.2461 - val_loss: 42117.6523\n",
      "Epoch 365/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27197.9453 - val_loss: 41981.4492\n",
      "Epoch 366/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27165.3418 - val_loss: 41833.5352\n",
      "Epoch 367/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 27304.1914 - val_loss: 42178.1680\n",
      "Epoch 368/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27326.7520 - val_loss: 41978.4727\n",
      "Epoch 369/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27200.0605 - val_loss: 43111.3438\n",
      "Epoch 370/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27261.4395 - val_loss: 41552.3164\n",
      "Epoch 371/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27498.5977 - val_loss: 41544.4961\n",
      "Epoch 372/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27434.5742 - val_loss: 41549.0000\n",
      "Epoch 373/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27265.6191 - val_loss: 41464.2070\n",
      "Epoch 374/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27352.9434 - val_loss: 41823.3125\n",
      "Epoch 375/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27404.5332 - val_loss: 41480.5508\n",
      "Epoch 376/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27051.6699 - val_loss: 41504.1719\n",
      "Epoch 377/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27217.6016 - val_loss: 41481.0273\n",
      "Epoch 378/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26943.8008 - val_loss: 41180.3633\n",
      "Epoch 379/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 27123.3574 - val_loss: 41217.9180\n",
      "Epoch 380/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26838.8594 - val_loss: 41897.3242\n",
      "Epoch 381/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26690.6230 - val_loss: 42221.6914\n",
      "Epoch 382/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 27019.2344 - val_loss: 41440.7148\n",
      "Epoch 383/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26912.5996 - val_loss: 41163.9766\n",
      "Epoch 384/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26913.7109 - val_loss: 41774.6914\n",
      "Epoch 385/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26435.7871 - val_loss: 41342.1992\n",
      "Epoch 386/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26872.1035 - val_loss: 41103.7773\n",
      "Epoch 387/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26607.1230 - val_loss: 42511.1289\n",
      "Epoch 388/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26910.3184 - val_loss: 42533.2578\n",
      "Epoch 389/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26338.8145 - val_loss: 41475.6055\n",
      "Epoch 390/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26574.4766 - val_loss: 41417.0859\n",
      "Epoch 391/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26541.9180 - val_loss: 41431.7773\n",
      "Epoch 392/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26234.3340 - val_loss: 41643.0273\n",
      "Epoch 393/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26530.9004 - val_loss: 41194.6641\n",
      "Epoch 394/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26744.3184 - val_loss: 40995.8750\n",
      "Epoch 395/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26364.4688 - val_loss: 41666.3164\n",
      "Epoch 396/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26736.2188 - val_loss: 41165.0586\n",
      "Epoch 397/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26457.3770 - val_loss: 41336.9414\n",
      "Epoch 398/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26620.3809 - val_loss: 40961.4141\n",
      "Epoch 399/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26591.4238 - val_loss: 41362.2070\n",
      "Epoch 400/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26030.1836 - val_loss: 40923.1172\n",
      "Epoch 401/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26102.3867 - val_loss: 41339.5234\n",
      "Epoch 402/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26281.2480 - val_loss: 40645.6719\n",
      "Epoch 403/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26121.1523 - val_loss: 42324.0430\n",
      "Epoch 404/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26441.7891 - val_loss: 40837.9180\n",
      "Epoch 405/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26043.0840 - val_loss: 40846.1914\n",
      "Epoch 406/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26196.9121 - val_loss: 42190.7539\n",
      "Epoch 407/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26370.0996 - val_loss: 40800.9414\n",
      "Epoch 408/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26055.1289 - val_loss: 40705.8125\n",
      "Epoch 409/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26262.5117 - val_loss: 40733.5156\n",
      "Epoch 410/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26045.3184 - val_loss: 40472.3320\n",
      "Epoch 411/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 26085.2598 - val_loss: 40877.5703\n",
      "Epoch 412/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 25793.6953 - val_loss: 40384.7500\n",
      "Epoch 413/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 25947.6992 - val_loss: 41414.3672\n",
      "Epoch 414/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 25853.0234 - val_loss: 40905.7812\n",
      "Epoch 415/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26127.7441 - val_loss: 40127.4297\n",
      "Epoch 416/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25965.6973 - val_loss: 40396.9141\n",
      "Epoch 417/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 26109.4277 - val_loss: 40155.7500\n",
      "Epoch 418/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 25631.8320 - val_loss: 40495.8477\n",
      "Epoch 419/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 25480.1133 - val_loss: 41762.2344\n",
      "Epoch 420/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 25850.3008 - val_loss: 41079.6289\n",
      "Epoch 421/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25769.2441 - val_loss: 40646.5547\n",
      "Epoch 422/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25941.7363 - val_loss: 40352.9492\n",
      "Epoch 423/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25390.1836 - val_loss: 40169.1133\n",
      "Epoch 424/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25292.5605 - val_loss: 42967.1914\n",
      "Epoch 425/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25262.7988 - val_loss: 39924.6016\n",
      "Epoch 426/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25805.0566 - val_loss: 40983.1719\n",
      "Epoch 427/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 26106.7441 - val_loss: 40682.7422\n",
      "Epoch 428/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25420.8672 - val_loss: 40192.5312\n",
      "Epoch 429/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25553.7559 - val_loss: 40118.0742\n",
      "Epoch 430/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25593.8477 - val_loss: 40005.1992\n",
      "Epoch 431/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25556.6992 - val_loss: 39861.7383\n",
      "Epoch 432/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25031.8418 - val_loss: 40747.1836\n",
      "Epoch 433/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24953.3086 - val_loss: 40316.7109\n",
      "Epoch 434/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25427.0898 - val_loss: 40379.7891\n",
      "Epoch 435/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25328.5742 - val_loss: 40073.3945\n",
      "Epoch 436/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25320.5879 - val_loss: 40756.9375\n",
      "Epoch 437/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25419.9590 - val_loss: 40674.8789\n",
      "Epoch 438/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25079.4844 - val_loss: 39850.1641\n",
      "Epoch 439/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24907.8301 - val_loss: 39889.1055\n",
      "Epoch 440/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25451.5605 - val_loss: 41437.1641\n",
      "Epoch 441/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24894.4863 - val_loss: 39746.5586\n",
      "Epoch 442/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24905.8867 - val_loss: 40750.1914\n",
      "Epoch 443/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25296.4941 - val_loss: 40481.6562\n",
      "Epoch 444/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24824.6348 - val_loss: 39588.5859\n",
      "Epoch 445/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25133.4023 - val_loss: 39462.0703\n",
      "Epoch 446/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 25307.5000 - val_loss: 39393.8086\n",
      "Epoch 447/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24819.9551 - val_loss: 39755.8086\n",
      "Epoch 448/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25508.3711 - val_loss: 41135.0547\n",
      "Epoch 449/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25008.1094 - val_loss: 39929.9180\n",
      "Epoch 450/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25088.1641 - val_loss: 39916.5312\n",
      "Epoch 451/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25330.1016 - val_loss: 41334.8867\n",
      "Epoch 452/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24637.9844 - val_loss: 43738.8398\n",
      "Epoch 453/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24782.4688 - val_loss: 39769.7773\n",
      "Epoch 454/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24747.8867 - val_loss: 39297.8164\n",
      "Epoch 455/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24822.3340 - val_loss: 39362.7031\n",
      "Epoch 456/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24554.7891 - val_loss: 39774.1914\n",
      "Epoch 457/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24627.2188 - val_loss: 39721.5117\n",
      "Epoch 458/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24789.2988 - val_loss: 39974.4297\n",
      "Epoch 459/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24619.2832 - val_loss: 39382.8320\n",
      "Epoch 460/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24576.5801 - val_loss: 39526.8438\n",
      "Epoch 461/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24574.8457 - val_loss: 39597.5625\n",
      "Epoch 462/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24947.9277 - val_loss: 39446.0352\n",
      "Epoch 463/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 25000.1191 - val_loss: 39325.2227\n",
      "Epoch 464/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24799.1719 - val_loss: 39236.3516\n",
      "Epoch 465/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24878.0547 - val_loss: 39847.5312\n",
      "Epoch 466/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24099.8438 - val_loss: 39481.0977\n",
      "Epoch 467/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24388.5117 - val_loss: 39552.6719\n",
      "Epoch 468/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24550.4863 - val_loss: 38921.0117\n",
      "Epoch 469/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24532.4121 - val_loss: 40288.1211\n",
      "Epoch 470/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24369.6543 - val_loss: 40418.5586\n",
      "Epoch 471/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24317.6172 - val_loss: 39544.1328\n",
      "Epoch 472/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24603.6934 - val_loss: 40393.4062\n",
      "Epoch 473/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24260.8066 - val_loss: 38978.3516\n",
      "Epoch 474/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 24361.0859 - val_loss: 39397.7031\n",
      "Epoch 475/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24090.2402 - val_loss: 38987.1289\n",
      "Epoch 476/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24177.2441 - val_loss: 38958.6289\n",
      "Epoch 477/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24081.1777 - val_loss: 40211.2227\n",
      "Epoch 478/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 24232.2383 - val_loss: 38974.1016\n",
      "Epoch 479/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 24336.4883 - val_loss: 41316.2070\n",
      "Epoch 480/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 23956.4043 - val_loss: 41611.8086\n",
      "Epoch 481/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 24277.0469 - val_loss: 39277.3672\n",
      "Epoch 482/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24309.3340 - val_loss: 38826.1758\n",
      "Epoch 483/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23974.3184 - val_loss: 39112.2070\n",
      "Epoch 484/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23882.9355 - val_loss: 38590.6289\n",
      "Epoch 485/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23991.1660 - val_loss: 39025.9844\n",
      "Epoch 486/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24121.9629 - val_loss: 38705.2930\n",
      "Epoch 487/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24028.9297 - val_loss: 38966.1836\n",
      "Epoch 488/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23719.6387 - val_loss: 38877.8750\n",
      "Epoch 489/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 24443.7617 - val_loss: 38881.0625\n",
      "Epoch 490/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 23443.4355 - val_loss: 38477.9570\n",
      "Epoch 491/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 24009.1719 - val_loss: 38632.4531\n",
      "Epoch 492/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23906.0664 - val_loss: 39390.0898\n",
      "Epoch 493/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 24319.1367 - val_loss: 39014.9961\n",
      "Epoch 494/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23728.9492 - val_loss: 39924.8438\n",
      "Epoch 495/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23729.4434 - val_loss: 38515.7070\n",
      "Epoch 496/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23813.9062 - val_loss: 38972.3359\n",
      "Epoch 497/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23961.8711 - val_loss: 39795.3438\n",
      "Epoch 498/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23793.3691 - val_loss: 38539.5352\n",
      "Epoch 499/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23618.0273 - val_loss: 38921.0703\n",
      "Epoch 500/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23837.3457 - val_loss: 39341.3633\n",
      "Epoch 501/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23819.0391 - val_loss: 39829.2617\n",
      "Epoch 502/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23740.1895 - val_loss: 38363.7656\n",
      "Epoch 503/1000\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 23456.2012 - val_loss: 39682.5547\n",
      "Epoch 504/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 23327.7793 - val_loss: 38994.1172\n",
      "Epoch 505/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 23135.5430 - val_loss: 38322.4062\n",
      "Epoch 506/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 24034.6680 - val_loss: 38159.5859\n",
      "Epoch 507/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 23576.6230 - val_loss: 38456.7148\n",
      "Epoch 508/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23776.3711 - val_loss: 38293.9570\n",
      "Epoch 509/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 23385.6309 - val_loss: 38308.8750\n",
      "Epoch 510/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23497.0742 - val_loss: 38262.8477\n",
      "Epoch 511/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23743.0898 - val_loss: 38331.8828\n",
      "Epoch 512/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23301.7324 - val_loss: 38031.8516\n",
      "Epoch 513/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23529.6699 - val_loss: 37970.2695\n",
      "Epoch 514/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23440.8535 - val_loss: 38331.1562\n",
      "Epoch 515/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23688.9902 - val_loss: 38204.5391\n",
      "Epoch 516/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22941.0723 - val_loss: 38633.4961\n",
      "Epoch 517/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23744.0137 - val_loss: 39428.0508\n",
      "Epoch 518/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23456.1719 - val_loss: 38111.4336\n",
      "Epoch 519/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23551.8320 - val_loss: 38227.5625\n",
      "Epoch 520/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23467.6953 - val_loss: 38083.4414\n",
      "Epoch 521/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23276.3457 - val_loss: 38410.4180\n",
      "Epoch 522/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23225.8281 - val_loss: 38127.6016\n",
      "Epoch 523/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23377.1367 - val_loss: 38483.3281\n",
      "Epoch 524/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23545.1172 - val_loss: 38828.6016\n",
      "Epoch 525/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23205.9355 - val_loss: 39450.1562\n",
      "Epoch 526/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23185.3984 - val_loss: 39539.5117\n",
      "Epoch 527/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23327.1484 - val_loss: 38196.2031\n",
      "Epoch 528/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22749.1055 - val_loss: 37936.7383\n",
      "Epoch 529/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23524.7344 - val_loss: 38006.3359\n",
      "Epoch 530/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23470.4824 - val_loss: 37986.2930\n",
      "Epoch 531/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22738.5215 - val_loss: 38206.6797\n",
      "Epoch 532/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23146.5137 - val_loss: 37641.0156\n",
      "Epoch 533/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23253.7402 - val_loss: 37719.2695\n",
      "Epoch 534/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23292.0488 - val_loss: 37768.3828\n",
      "Epoch 535/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 23359.4648 - val_loss: 38817.6445\n",
      "Epoch 536/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23165.7617 - val_loss: 38082.9805\n",
      "Epoch 537/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 22838.7285 - val_loss: 38236.4453\n",
      "Epoch 538/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 23155.6660 - val_loss: 37796.2852\n",
      "Epoch 539/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23129.8867 - val_loss: 37599.1367\n",
      "Epoch 540/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22868.1719 - val_loss: 37506.0977\n",
      "Epoch 541/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22843.8535 - val_loss: 37407.3789\n",
      "Epoch 542/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 22929.3262 - val_loss: 37765.7148\n",
      "Epoch 543/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22512.5117 - val_loss: 39947.9336\n",
      "Epoch 544/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 22963.6445 - val_loss: 38223.4727\n",
      "Epoch 545/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23262.0527 - val_loss: 38049.6445\n",
      "Epoch 546/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23183.6172 - val_loss: 37763.5469\n",
      "Epoch 547/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 22597.1035 - val_loss: 38161.9336\n",
      "Epoch 548/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 22964.0000 - val_loss: 39572.3789\n",
      "Epoch 549/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 23284.3242 - val_loss: 37972.3984\n",
      "Epoch 550/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 22700.1699 - val_loss: 37331.1836\n",
      "Epoch 551/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22697.9863 - val_loss: 37473.7227\n",
      "Epoch 552/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22679.1367 - val_loss: 37844.1953\n",
      "Epoch 553/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22814.2969 - val_loss: 37421.4141\n",
      "Epoch 554/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 23038.3281 - val_loss: 37457.6367\n",
      "Epoch 555/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22485.2832 - val_loss: 37673.4297\n",
      "Epoch 556/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22319.7383 - val_loss: 37503.1758\n",
      "Epoch 557/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22288.0488 - val_loss: 37111.7695\n",
      "Epoch 558/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22917.8438 - val_loss: 38616.5117\n",
      "Epoch 559/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22696.7695 - val_loss: 37488.6250\n",
      "Epoch 560/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22458.7246 - val_loss: 37201.8047\n",
      "Epoch 561/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22507.1406 - val_loss: 37178.4648\n",
      "Epoch 562/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22569.1973 - val_loss: 37122.9648\n",
      "Epoch 563/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22301.9785 - val_loss: 37750.2109\n",
      "Epoch 564/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22508.4688 - val_loss: 39565.2070\n",
      "Epoch 565/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22822.8672 - val_loss: 37090.5312\n",
      "Epoch 566/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22497.6445 - val_loss: 37162.7031\n",
      "Epoch 567/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22526.8184 - val_loss: 37108.3164\n",
      "Epoch 568/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22768.9766 - val_loss: 37226.9531\n",
      "Epoch 569/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22405.9180 - val_loss: 37060.1719\n",
      "Epoch 570/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22088.6465 - val_loss: 37801.7461\n",
      "Epoch 571/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22626.6621 - val_loss: 36913.8789\n",
      "Epoch 572/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22412.2930 - val_loss: 38524.3828\n",
      "Epoch 573/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22628.8945 - val_loss: 36938.2617\n",
      "Epoch 574/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22342.5332 - val_loss: 39590.1211\n",
      "Epoch 575/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22501.3086 - val_loss: 37352.2031\n",
      "Epoch 576/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22565.7617 - val_loss: 36932.5156\n",
      "Epoch 577/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22695.6562 - val_loss: 37119.7070\n",
      "Epoch 578/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22195.7227 - val_loss: 38057.3789\n",
      "Epoch 579/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22303.6914 - val_loss: 37332.4180\n",
      "Epoch 580/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22926.7188 - val_loss: 37306.1133\n",
      "Epoch 581/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22559.2598 - val_loss: 36690.7969\n",
      "Epoch 582/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22317.8203 - val_loss: 36837.6719\n",
      "Epoch 583/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22260.3887 - val_loss: 37444.7617\n",
      "Epoch 584/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22664.6328 - val_loss: 38650.7617\n",
      "Epoch 585/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21968.6328 - val_loss: 36690.3945\n",
      "Epoch 586/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21981.2402 - val_loss: 36897.9492\n",
      "Epoch 587/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22280.9766 - val_loss: 37320.5508\n",
      "Epoch 588/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 21666.6113 - val_loss: 36762.6172\n",
      "Epoch 589/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22044.5430 - val_loss: 36765.3203\n",
      "Epoch 590/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 22323.9980 - val_loss: 37358.0781\n",
      "Epoch 591/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22179.2695 - val_loss: 37407.6055\n",
      "Epoch 592/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 22300.5000 - val_loss: 37697.3047\n",
      "Epoch 593/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22413.9941 - val_loss: 36969.1211\n",
      "Epoch 594/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21407.7969 - val_loss: 36867.5391\n",
      "Epoch 595/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21931.5977 - val_loss: 38092.0117\n",
      "Epoch 596/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22119.0391 - val_loss: 37520.2383\n",
      "Epoch 597/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22069.5078 - val_loss: 38544.0859\n",
      "Epoch 598/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21922.9746 - val_loss: 36689.4336\n",
      "Epoch 599/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22158.8223 - val_loss: 36488.8398\n",
      "Epoch 600/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22253.6875 - val_loss: 36711.2227\n",
      "Epoch 601/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21855.3594 - val_loss: 36544.4219\n",
      "Epoch 602/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22392.7852 - val_loss: 36516.7852\n",
      "Epoch 603/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21984.9590 - val_loss: 36437.8867\n",
      "Epoch 604/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21996.1348 - val_loss: 36666.7891\n",
      "Epoch 605/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21879.5410 - val_loss: 38351.0820\n",
      "Epoch 606/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22523.8887 - val_loss: 39949.6953\n",
      "Epoch 607/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22095.9473 - val_loss: 38395.7070\n",
      "Epoch 608/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22451.9746 - val_loss: 36530.7227\n",
      "Epoch 609/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22194.8301 - val_loss: 36996.2695\n",
      "Epoch 610/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22676.0625 - val_loss: 37622.3984\n",
      "Epoch 611/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21654.1113 - val_loss: 36874.3867\n",
      "Epoch 612/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21615.2812 - val_loss: 37265.3320\n",
      "Epoch 613/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22170.6992 - val_loss: 39317.3242\n",
      "Epoch 614/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21891.7969 - val_loss: 36850.3789\n",
      "Epoch 615/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21504.5449 - val_loss: 38358.7734\n",
      "Epoch 616/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22086.5840 - val_loss: 36934.3281\n",
      "Epoch 617/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21417.9160 - val_loss: 37188.0859\n",
      "Epoch 618/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21678.1875 - val_loss: 36721.5977\n",
      "Epoch 619/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21656.0449 - val_loss: 37645.8711\n",
      "Epoch 620/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22225.7598 - val_loss: 36527.0625\n",
      "Epoch 621/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21608.7148 - val_loss: 36401.7656\n",
      "Epoch 622/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21722.4707 - val_loss: 36720.7109\n",
      "Epoch 623/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21641.2051 - val_loss: 36357.5898\n",
      "Epoch 624/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21858.3301 - val_loss: 38152.4570\n",
      "Epoch 625/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22608.8086 - val_loss: 36281.5781\n",
      "Epoch 626/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21422.2793 - val_loss: 36444.0859\n",
      "Epoch 627/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21609.5664 - val_loss: 36983.8516\n",
      "Epoch 628/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21529.5352 - val_loss: 36115.5000\n",
      "Epoch 629/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21739.9395 - val_loss: 36912.7305\n",
      "Epoch 630/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21311.9902 - val_loss: 37005.9727\n",
      "Epoch 631/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21923.4844 - val_loss: 36577.3828\n",
      "Epoch 632/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21221.9219 - val_loss: 36283.6328\n",
      "Epoch 633/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21456.8398 - val_loss: 36305.3281\n",
      "Epoch 634/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21318.6992 - val_loss: 36163.6523\n",
      "Epoch 635/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21353.5293 - val_loss: 36263.6719\n",
      "Epoch 636/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21624.3555 - val_loss: 36470.3789\n",
      "Epoch 637/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 21314.6309 - val_loss: 36429.7305\n",
      "Epoch 638/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21454.9609 - val_loss: 38613.1680\n",
      "Epoch 639/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21207.2363 - val_loss: 36833.2578\n",
      "Epoch 640/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 22059.9277 - val_loss: 35809.6914\n",
      "Epoch 641/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21321.9238 - val_loss: 36740.0156\n",
      "Epoch 642/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21547.7734 - val_loss: 37921.6016\n",
      "Epoch 643/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21720.0332 - val_loss: 38413.5977\n",
      "Epoch 644/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 22122.2930 - val_loss: 35852.0508\n",
      "Epoch 645/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20782.9980 - val_loss: 36567.6211\n",
      "Epoch 646/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20840.9805 - val_loss: 36419.1133\n",
      "Epoch 647/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21434.3027 - val_loss: 37175.0508\n",
      "Epoch 648/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21434.1973 - val_loss: 36679.5234\n",
      "Epoch 649/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21377.4199 - val_loss: 36494.0430\n",
      "Epoch 650/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21017.8438 - val_loss: 36663.2656\n",
      "Epoch 651/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21151.0332 - val_loss: 37737.6602\n",
      "Epoch 652/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21455.6562 - val_loss: 36609.9102\n",
      "Epoch 653/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21453.7578 - val_loss: 36283.7422\n",
      "Epoch 654/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21270.5352 - val_loss: 36380.3359\n",
      "Epoch 655/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21054.3438 - val_loss: 36226.1641\n",
      "Epoch 656/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20830.9160 - val_loss: 36618.5703\n",
      "Epoch 657/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21168.7871 - val_loss: 35854.2539\n",
      "Epoch 658/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21394.3320 - val_loss: 36766.4727\n",
      "Epoch 659/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21545.2695 - val_loss: 35755.6914\n",
      "Epoch 660/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21104.3008 - val_loss: 35661.1641\n",
      "Epoch 661/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21824.6758 - val_loss: 35799.2305\n",
      "Epoch 662/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20739.2832 - val_loss: 37032.6484\n",
      "Epoch 663/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21077.1602 - val_loss: 35931.1133\n",
      "Epoch 664/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20612.9492 - val_loss: 35629.4258\n",
      "Epoch 665/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20712.6816 - val_loss: 36045.8242\n",
      "Epoch 666/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20896.4023 - val_loss: 36222.8438\n",
      "Epoch 667/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21594.6270 - val_loss: 35813.0195\n",
      "Epoch 668/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20710.1660 - val_loss: 36278.7305\n",
      "Epoch 669/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21612.9297 - val_loss: 38733.6992\n",
      "Epoch 670/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 20813.6133 - val_loss: 35880.3242\n",
      "Epoch 671/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20936.8320 - val_loss: 35815.3984\n",
      "Epoch 672/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20699.5117 - val_loss: 35985.0586\n",
      "Epoch 673/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21150.0215 - val_loss: 35736.3906\n",
      "Epoch 674/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21128.5449 - val_loss: 35681.7148\n",
      "Epoch 675/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20992.9297 - val_loss: 36279.4492\n",
      "Epoch 676/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21228.1699 - val_loss: 35912.5234\n",
      "Epoch 677/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21406.9473 - val_loss: 35875.6133\n",
      "Epoch 678/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20662.5488 - val_loss: 36836.3281\n",
      "Epoch 679/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20768.4746 - val_loss: 37901.8359\n",
      "Epoch 680/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20799.3750 - val_loss: 36059.2734\n",
      "Epoch 681/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20611.9902 - val_loss: 35697.4805\n",
      "Epoch 682/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20560.0195 - val_loss: 36168.3555\n",
      "Epoch 683/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20974.1270 - val_loss: 35686.0352\n",
      "Epoch 684/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20849.1621 - val_loss: 35587.8086\n",
      "Epoch 685/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20516.9277 - val_loss: 35675.8047\n",
      "Epoch 686/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20793.8848 - val_loss: 35835.9141\n",
      "Epoch 687/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21039.7754 - val_loss: 35744.5508\n",
      "Epoch 688/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20901.4473 - val_loss: 35702.0273\n",
      "Epoch 689/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20818.3672 - val_loss: 36426.6094\n",
      "Epoch 690/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20608.0645 - val_loss: 37025.5742\n",
      "Epoch 691/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21565.3105 - val_loss: 35922.5195\n",
      "Epoch 692/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20846.4062 - val_loss: 36418.4102\n",
      "Epoch 693/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20529.6289 - val_loss: 38077.6992\n",
      "Epoch 694/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20836.5156 - val_loss: 35531.7031\n",
      "Epoch 695/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20617.9316 - val_loss: 36686.6875\n",
      "Epoch 696/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20956.0801 - val_loss: 36466.6602\n",
      "Epoch 697/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20314.2559 - val_loss: 35696.8242\n",
      "Epoch 698/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20756.7910 - val_loss: 35474.8359\n",
      "Epoch 699/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20643.1465 - val_loss: 36617.5859\n",
      "Epoch 700/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20748.2891 - val_loss: 37628.1250\n",
      "Epoch 701/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20901.3750 - val_loss: 36120.5078\n",
      "Epoch 702/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20217.6855 - val_loss: 36361.1211\n",
      "Epoch 703/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20785.2383 - val_loss: 36153.6797\n",
      "Epoch 704/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20567.8496 - val_loss: 35633.1680\n",
      "Epoch 705/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 21325.6855 - val_loss: 36086.8516\n",
      "Epoch 706/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 21207.8047 - val_loss: 36037.6016\n",
      "Epoch 707/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20455.2617 - val_loss: 35751.0000\n",
      "Epoch 708/1000\n",
      "114/114 [==============================] - ETA: 0s - loss: 21015.210 - 0s 2ms/step - loss: 20487.6406 - val_loss: 35689.0938\n",
      "Epoch 709/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20734.3262 - val_loss: 35759.5977\n",
      "Epoch 710/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 20134.3555 - val_loss: 36557.2930\n",
      "Epoch 711/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20642.8789 - val_loss: 35809.5703\n",
      "Epoch 712/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20314.2383 - val_loss: 35605.4297\n",
      "Epoch 713/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20775.5254 - val_loss: 35887.2305\n",
      "Epoch 714/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20773.1953 - val_loss: 36746.5469\n",
      "Epoch 715/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20087.0176 - val_loss: 36055.6133\n",
      "Epoch 716/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20450.7129 - val_loss: 35675.2227\n",
      "Epoch 717/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20575.5352 - val_loss: 35307.3711\n",
      "Epoch 718/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20769.2441 - val_loss: 35613.4297\n",
      "Epoch 719/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20855.4785 - val_loss: 35564.2148\n",
      "Epoch 720/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20138.5918 - val_loss: 36049.3281\n",
      "Epoch 721/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 21077.3867 - val_loss: 35419.1836\n",
      "Epoch 722/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20677.9043 - val_loss: 36472.2344\n",
      "Epoch 723/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20370.5312 - val_loss: 35844.5703\n",
      "Epoch 724/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20320.7344 - val_loss: 37011.5430\n",
      "Epoch 725/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20345.9277 - val_loss: 35068.6211\n",
      "Epoch 726/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20392.6211 - val_loss: 35482.4531\n",
      "Epoch 727/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20330.9902 - val_loss: 35951.7344\n",
      "Epoch 728/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20420.0078 - val_loss: 35855.7891\n",
      "Epoch 729/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20198.6309 - val_loss: 35406.0000\n",
      "Epoch 730/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20025.6875 - val_loss: 36517.4688\n",
      "Epoch 731/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20256.7520 - val_loss: 37882.7031\n",
      "Epoch 732/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20536.5547 - val_loss: 35418.1875\n",
      "Epoch 733/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19935.9023 - val_loss: 35462.5781\n",
      "Epoch 734/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20761.7617 - val_loss: 36160.7500\n",
      "Epoch 735/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20301.1387 - val_loss: 36241.0273\n",
      "Epoch 736/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 20468.7031 - val_loss: 36102.7891\n",
      "Epoch 737/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19982.0977 - val_loss: 35612.9297\n",
      "Epoch 738/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20592.4551 - val_loss: 36208.7656\n",
      "Epoch 739/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20378.9648 - val_loss: 35538.8281\n",
      "Epoch 740/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19914.2754 - val_loss: 35677.8086\n",
      "Epoch 741/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19789.8477 - val_loss: 35421.1445\n",
      "Epoch 742/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20304.0938 - val_loss: 35480.5078\n",
      "Epoch 743/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20500.6152 - val_loss: 36301.4766\n",
      "Epoch 744/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19675.1582 - val_loss: 35843.2148\n",
      "Epoch 745/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20114.7676 - val_loss: 35262.6445\n",
      "Epoch 746/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20323.5645 - val_loss: 35008.6484\n",
      "Epoch 747/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 20272.4160 - val_loss: 36251.1641\n",
      "Epoch 748/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19797.1992 - val_loss: 36350.5234\n",
      "Epoch 749/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20252.4004 - val_loss: 38294.2148\n",
      "Epoch 750/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20796.1367 - val_loss: 35144.3438\n",
      "Epoch 751/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20357.1855 - val_loss: 36004.6641\n",
      "Epoch 752/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20049.2910 - val_loss: 35993.0586\n",
      "Epoch 753/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 20355.1426 - val_loss: 35564.8086\n",
      "Epoch 754/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19725.6816 - val_loss: 36789.7031\n",
      "Epoch 755/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20299.2129 - val_loss: 35524.4531\n",
      "Epoch 756/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19624.9121 - val_loss: 35298.9180\n",
      "Epoch 757/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19535.1641 - val_loss: 36130.2461\n",
      "Epoch 758/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20251.7578 - val_loss: 35859.0742\n",
      "Epoch 759/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20510.5527 - val_loss: 35562.8594\n",
      "Epoch 760/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20157.1660 - val_loss: 36267.0312\n",
      "Epoch 761/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20044.9062 - val_loss: 35418.1328\n",
      "Epoch 762/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19860.8945 - val_loss: 35345.0312\n",
      "Epoch 763/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19795.9199 - val_loss: 36106.5820\n",
      "Epoch 764/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 20751.6367 - val_loss: 35593.3555\n",
      "Epoch 765/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20074.2656 - val_loss: 35399.3398\n",
      "Epoch 766/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19543.9238 - val_loss: 35596.8789\n",
      "Epoch 767/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19684.7480 - val_loss: 35542.7031\n",
      "Epoch 768/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20511.1582 - val_loss: 37800.1484\n",
      "Epoch 769/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19954.5078 - val_loss: 36968.0039\n",
      "Epoch 770/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19506.7832 - val_loss: 35773.9062\n",
      "Epoch 771/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19979.3066 - val_loss: 35577.3281\n",
      "Epoch 772/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19648.4277 - val_loss: 35136.4180\n",
      "Epoch 773/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19746.3105 - val_loss: 35426.1211\n",
      "Epoch 774/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19826.9062 - val_loss: 35072.0859\n",
      "Epoch 775/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20586.6758 - val_loss: 34965.7734\n",
      "Epoch 776/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19672.1328 - val_loss: 35151.3281\n",
      "Epoch 777/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19906.0781 - val_loss: 35871.9922\n",
      "Epoch 778/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19700.8984 - val_loss: 35552.6562\n",
      "Epoch 779/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19892.9121 - val_loss: 37717.1602\n",
      "Epoch 780/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 19777.6211 - val_loss: 35193.5664\n",
      "Epoch 781/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19627.8516 - val_loss: 35240.9688\n",
      "Epoch 782/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19915.9727 - val_loss: 35554.8359\n",
      "Epoch 783/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19300.5039 - val_loss: 35157.8398\n",
      "Epoch 784/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20128.6699 - val_loss: 34873.1797\n",
      "Epoch 785/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 20265.3867 - val_loss: 35478.2383\n",
      "Epoch 786/1000\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 20324.5137 - val_loss: 36178.8945\n",
      "Epoch 787/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19627.8691 - val_loss: 34831.5977\n",
      "Epoch 788/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19832.6953 - val_loss: 35619.2422\n",
      "Epoch 789/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19831.0684 - val_loss: 37029.4648\n",
      "Epoch 790/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19397.5410 - val_loss: 34931.6641\n",
      "Epoch 791/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19920.5312 - val_loss: 35020.3633\n",
      "Epoch 792/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19734.6582 - val_loss: 36945.2109\n",
      "Epoch 793/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19968.8164 - val_loss: 34691.0508\n",
      "Epoch 794/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18985.1074 - val_loss: 35023.0469\n",
      "Epoch 795/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19396.2383 - val_loss: 35152.0312\n",
      "Epoch 796/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19981.3164 - val_loss: 35138.5703\n",
      "Epoch 797/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19701.7246 - val_loss: 36183.1953\n",
      "Epoch 798/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19629.0312 - val_loss: 37611.9258\n",
      "Epoch 799/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 20131.7695 - val_loss: 36491.6562\n",
      "Epoch 800/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19651.0098 - val_loss: 34821.2617\n",
      "Epoch 801/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19174.6035 - val_loss: 35161.4922\n",
      "Epoch 802/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19432.3789 - val_loss: 34954.5508\n",
      "Epoch 803/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19571.7637 - val_loss: 34671.1836\n",
      "Epoch 804/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19385.3438 - val_loss: 34615.1562\n",
      "Epoch 805/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19424.0605 - val_loss: 35037.2930\n",
      "Epoch 806/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19373.4902 - val_loss: 35386.9922\n",
      "Epoch 807/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19571.9531 - val_loss: 35218.8320\n",
      "Epoch 808/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19448.3984 - val_loss: 34685.9219\n",
      "Epoch 809/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19547.9609 - val_loss: 35837.6836\n",
      "Epoch 810/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19060.0332 - val_loss: 36746.0430\n",
      "Epoch 811/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19781.8672 - val_loss: 35852.9727\n",
      "Epoch 812/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20226.8887 - val_loss: 35061.6641\n",
      "Epoch 813/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19258.9297 - val_loss: 35506.0312\n",
      "Epoch 814/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19930.8379 - val_loss: 35008.4219\n",
      "Epoch 815/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19145.3848 - val_loss: 36091.5352\n",
      "Epoch 816/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19626.0488 - val_loss: 34991.9844\n",
      "Epoch 817/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19245.8984 - val_loss: 36232.3594\n",
      "Epoch 818/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19642.3457 - val_loss: 34494.6992\n",
      "Epoch 819/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19674.3320 - val_loss: 34625.6719\n",
      "Epoch 820/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19315.3145 - val_loss: 37585.7227\n",
      "Epoch 821/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19438.8086 - val_loss: 35080.6953\n",
      "Epoch 822/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19197.7734 - val_loss: 34818.9766\n",
      "Epoch 823/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19441.6914 - val_loss: 36773.9922\n",
      "Epoch 824/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19421.8711 - val_loss: 34767.8125\n",
      "Epoch 825/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19458.0605 - val_loss: 34818.2617\n",
      "Epoch 826/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19497.5488 - val_loss: 35185.8867\n",
      "Epoch 827/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19600.7363 - val_loss: 35251.4570\n",
      "Epoch 828/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19194.3984 - val_loss: 34991.0078\n",
      "Epoch 829/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19674.8984 - val_loss: 37310.2539\n",
      "Epoch 830/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 20080.3828 - val_loss: 35285.7695\n",
      "Epoch 831/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19300.8750 - val_loss: 34888.0664\n",
      "Epoch 832/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19521.3965 - val_loss: 35781.2539\n",
      "Epoch 833/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 19624.1348 - val_loss: 34699.1406\n",
      "Epoch 834/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 18998.7988 - val_loss: 35066.2930\n",
      "Epoch 835/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19559.9297 - val_loss: 34782.0547\n",
      "Epoch 836/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19103.4648 - val_loss: 35865.5469\n",
      "Epoch 837/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19145.9512 - val_loss: 34591.3164\n",
      "Epoch 838/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19145.3887 - val_loss: 34805.6797\n",
      "Epoch 839/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19528.2754 - val_loss: 35004.2969\n",
      "Epoch 840/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19244.9668 - val_loss: 35229.7930\n",
      "Epoch 841/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 19252.3965 - val_loss: 35441.4570\n",
      "Epoch 842/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19243.7227 - val_loss: 35963.3945\n",
      "Epoch 843/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19015.0254 - val_loss: 35314.0391\n",
      "Epoch 844/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 19248.0156 - val_loss: 35589.0391\n",
      "Epoch 845/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19343.2422 - val_loss: 35732.7109\n",
      "Epoch 846/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 18996.4961 - val_loss: 35223.5703\n",
      "Epoch 847/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 18983.7539 - val_loss: 35155.4805\n",
      "Epoch 848/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 19023.0098 - val_loss: 34967.4570\n",
      "Epoch 849/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18604.3145 - val_loss: 35129.0000\n",
      "Epoch 850/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19121.0527 - val_loss: 34801.3125\n",
      "Epoch 851/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18780.7461 - val_loss: 34740.9883\n",
      "Epoch 852/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18932.1504 - val_loss: 34854.3828\n",
      "Epoch 853/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19137.3730 - val_loss: 34832.8242\n",
      "Epoch 854/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18796.7637 - val_loss: 34760.5547\n",
      "Epoch 855/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19330.5566 - val_loss: 34801.0664\n",
      "Epoch 856/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19355.8223 - val_loss: 35520.9219\n",
      "Epoch 857/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18942.0586 - val_loss: 35133.9609\n",
      "Epoch 858/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19063.6738 - val_loss: 36060.6367\n",
      "Epoch 859/1000\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 19094.4102 - val_loss: 35542.4180\n",
      "Epoch 860/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19142.5039 - val_loss: 34738.7930\n",
      "Epoch 861/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18664.5137 - val_loss: 34723.5430\n",
      "Epoch 862/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19222.2930 - val_loss: 34477.9141\n",
      "Epoch 863/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18941.3242 - val_loss: 34543.3398\n",
      "Epoch 864/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 19122.9004 - val_loss: 35557.2188\n",
      "Epoch 865/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18916.3633 - val_loss: 35056.5391\n",
      "Epoch 866/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18851.6426 - val_loss: 35592.0625\n",
      "Epoch 867/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18931.3574 - val_loss: 35247.9766\n",
      "Epoch 868/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19306.2988 - val_loss: 36754.9102\n",
      "Epoch 869/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19583.3887 - val_loss: 35973.0195\n",
      "Epoch 870/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19240.1230 - val_loss: 34501.9375\n",
      "Epoch 871/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18688.1816 - val_loss: 36042.3125\n",
      "Epoch 872/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19162.3477 - val_loss: 35009.4258\n",
      "Epoch 873/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18736.9902 - val_loss: 36780.4688\n",
      "Epoch 874/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19171.6895 - val_loss: 35334.2734\n",
      "Epoch 875/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18631.6465 - val_loss: 35007.5703\n",
      "Epoch 876/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18637.9844 - val_loss: 35445.1016\n",
      "Epoch 877/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19059.5586 - val_loss: 36641.6797\n",
      "Epoch 878/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19229.3086 - val_loss: 34730.3555\n",
      "Epoch 879/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18872.5566 - val_loss: 34559.2266\n",
      "Epoch 880/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18829.2500 - val_loss: 35095.8008\n",
      "Epoch 881/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19001.3887 - val_loss: 34878.4258\n",
      "Epoch 882/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19338.1602 - val_loss: 35144.9609\n",
      "Epoch 883/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19110.5898 - val_loss: 34520.3945\n",
      "Epoch 884/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19407.6523 - val_loss: 35315.3867\n",
      "Epoch 885/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19310.8203 - val_loss: 36639.0938\n",
      "Epoch 886/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18974.7559 - val_loss: 35632.9844\n",
      "Epoch 887/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18624.8750 - val_loss: 34854.7031\n",
      "Epoch 888/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18537.3027 - val_loss: 36738.6367\n",
      "Epoch 889/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18551.3730 - val_loss: 34896.0820\n",
      "Epoch 890/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18793.1953 - val_loss: 34978.9844\n",
      "Epoch 891/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18812.6660 - val_loss: 34708.0781\n",
      "Epoch 892/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18399.3047 - val_loss: 34390.7070\n",
      "Epoch 893/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18674.7031 - val_loss: 34723.3828\n",
      "Epoch 894/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18306.4023 - val_loss: 34491.6250\n",
      "Epoch 895/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18618.0176 - val_loss: 34719.0234\n",
      "Epoch 896/1000\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 18757.4492 - val_loss: 37682.2930\n",
      "Epoch 897/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19000.3691 - val_loss: 35189.1680\n",
      "Epoch 898/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18412.0508 - val_loss: 34414.4492\n",
      "Epoch 899/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18787.8965 - val_loss: 34702.9609\n",
      "Epoch 900/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18841.4785 - val_loss: 34760.1602\n",
      "Epoch 901/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18535.1230 - val_loss: 34904.4297\n",
      "Epoch 902/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18436.0254 - val_loss: 34684.5938\n",
      "Epoch 903/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18875.2949 - val_loss: 34647.7695\n",
      "Epoch 904/1000\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 18416.0195 - val_loss: 34792.7500\n",
      "Epoch 905/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18291.8613 - val_loss: 34552.9688\n",
      "Epoch 906/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18472.2734 - val_loss: 35568.0391\n",
      "Epoch 907/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18362.6094 - val_loss: 35272.0508\n",
      "Epoch 908/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18685.9551 - val_loss: 34890.3477\n",
      "Epoch 909/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18922.0430 - val_loss: 34626.0312\n",
      "Epoch 910/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19032.0859 - val_loss: 35698.5742\n",
      "Epoch 911/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18473.4688 - val_loss: 35338.1133\n",
      "Epoch 912/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18682.7168 - val_loss: 34658.7695\n",
      "Epoch 913/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18131.7578 - val_loss: 35341.5820\n",
      "Epoch 914/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18267.2520 - val_loss: 35532.2617\n",
      "Epoch 915/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18485.5781 - val_loss: 35609.8750\n",
      "Epoch 916/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18701.8047 - val_loss: 36109.3828\n",
      "Epoch 917/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18381.0684 - val_loss: 34954.5430\n",
      "Epoch 918/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18185.2383 - val_loss: 34598.3086\n",
      "Epoch 919/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18152.0293 - val_loss: 37068.2188\n",
      "Epoch 920/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18636.8809 - val_loss: 34123.9844\n",
      "Epoch 921/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18395.7207 - val_loss: 34468.6680\n",
      "Epoch 922/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18542.6914 - val_loss: 34879.7344\n",
      "Epoch 923/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18341.8594 - val_loss: 34571.0508\n",
      "Epoch 924/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18481.7070 - val_loss: 34642.1719\n",
      "Epoch 925/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18390.1309 - val_loss: 34982.2578\n",
      "Epoch 926/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18543.6328 - val_loss: 35547.5781\n",
      "Epoch 927/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17936.1562 - val_loss: 35773.9375\n",
      "Epoch 928/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18527.1660 - val_loss: 34374.8594\n",
      "Epoch 929/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18883.8359 - val_loss: 34576.4336\n",
      "Epoch 930/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18309.3887 - val_loss: 35073.8438\n",
      "Epoch 931/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18553.9551 - val_loss: 35666.3203\n",
      "Epoch 932/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18821.0117 - val_loss: 34687.7930\n",
      "Epoch 933/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18444.9629 - val_loss: 34498.7148\n",
      "Epoch 934/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19694.3066 - val_loss: 34403.1367\n",
      "Epoch 935/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17944.2090 - val_loss: 34456.0820\n",
      "Epoch 936/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17979.5312 - val_loss: 34473.8867\n",
      "Epoch 937/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18367.6758 - val_loss: 36766.9688\n",
      "Epoch 938/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18918.3125 - val_loss: 34588.6484\n",
      "Epoch 939/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18118.6953 - val_loss: 34463.5938\n",
      "Epoch 940/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17758.2227 - val_loss: 35048.2930\n",
      "Epoch 941/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18705.3965 - val_loss: 37318.3945\n",
      "Epoch 942/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 19009.0391 - val_loss: 34709.1328\n",
      "Epoch 943/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18303.8320 - val_loss: 36855.1953\n",
      "Epoch 944/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18348.4395 - val_loss: 36158.1523\n",
      "Epoch 945/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18355.2090 - val_loss: 34335.7383\n",
      "Epoch 946/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18840.9629 - val_loss: 34304.7656\n",
      "Epoch 947/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17921.3730 - val_loss: 35347.7383\n",
      "Epoch 948/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18422.0176 - val_loss: 34718.0117\n",
      "Epoch 949/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18496.4590 - val_loss: 34932.4844\n",
      "Epoch 950/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17996.8516 - val_loss: 36016.1172\n",
      "Epoch 951/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18101.9141 - val_loss: 35789.6484\n",
      "Epoch 952/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18244.7500 - val_loss: 34973.2617\n",
      "Epoch 953/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 18053.5254 - val_loss: 34610.8008\n",
      "Epoch 954/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 18335.1406 - val_loss: 34468.2188\n",
      "Epoch 955/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 17965.6992 - val_loss: 34573.2305\n",
      "Epoch 956/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18028.5664 - val_loss: 35232.4492\n",
      "Epoch 957/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18392.4004 - val_loss: 34657.2344\n",
      "Epoch 958/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 17753.6191 - val_loss: 34632.4062\n",
      "Epoch 959/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18183.1387 - val_loss: 35205.7344\n",
      "Epoch 960/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 17915.3027 - val_loss: 34263.9258\n",
      "Epoch 961/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 18768.0508 - val_loss: 34807.8828\n",
      "Epoch 962/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 17989.8672 - val_loss: 35299.3516\n",
      "Epoch 963/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18201.0156 - val_loss: 34812.7852\n",
      "Epoch 964/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 18026.8457 - val_loss: 34511.4219\n",
      "Epoch 965/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 17909.2852 - val_loss: 35675.9102\n",
      "Epoch 966/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 17808.1621 - val_loss: 34497.8984\n",
      "Epoch 967/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 17776.9609 - val_loss: 34439.4336\n",
      "Epoch 968/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 18494.2188 - val_loss: 34486.5547\n",
      "Epoch 969/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 17727.5000 - val_loss: 35055.2422\n",
      "Epoch 970/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18054.3867 - val_loss: 34151.5664\n",
      "Epoch 971/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17903.9258 - val_loss: 34938.6211\n",
      "Epoch 972/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 17530.1348 - val_loss: 34635.7812\n",
      "Epoch 973/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17533.0820 - val_loss: 34433.3047\n",
      "Epoch 974/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18159.7715 - val_loss: 34440.7422\n",
      "Epoch 975/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18498.6309 - val_loss: 34218.1680\n",
      "Epoch 976/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17507.8301 - val_loss: 34508.3789\n",
      "Epoch 977/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18258.9355 - val_loss: 34792.1445\n",
      "Epoch 978/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17636.5547 - val_loss: 34260.8789\n",
      "Epoch 979/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18277.0352 - val_loss: 35095.6992\n",
      "Epoch 980/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18231.3828 - val_loss: 34572.2773\n",
      "Epoch 981/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17916.5840 - val_loss: 34628.0586\n",
      "Epoch 982/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18191.7383 - val_loss: 34254.7656\n",
      "Epoch 983/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17606.3633 - val_loss: 36775.0273\n",
      "Epoch 984/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18168.2949 - val_loss: 40046.4180\n",
      "Epoch 985/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18495.9062 - val_loss: 34891.2734\n",
      "Epoch 986/1000\n",
      "114/114 [==============================] - 0s 3ms/step - loss: 17930.0820 - val_loss: 34415.1836\n",
      "Epoch 987/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17742.4023 - val_loss: 34401.6484\n",
      "Epoch 988/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18030.2344 - val_loss: 35518.0703\n",
      "Epoch 989/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18378.5098 - val_loss: 34675.5703\n",
      "Epoch 990/1000\n",
      "114/114 [==============================] - 1s 4ms/step - loss: 17997.0391 - val_loss: 34801.2578\n",
      "Epoch 991/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 18138.9141 - val_loss: 35894.8164\n",
      "Epoch 992/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 17770.7344 - val_loss: 34410.7461\n",
      "Epoch 993/1000\n",
      "114/114 [==============================] - 0s 4ms/step - loss: 18198.6367 - val_loss: 35159.2227\n",
      "Epoch 994/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17489.1855 - val_loss: 34298.5664\n",
      "Epoch 995/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18044.6074 - val_loss: 34268.2031\n",
      "Epoch 996/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 17450.8848 - val_loss: 34853.9609\n",
      "Epoch 997/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17845.0879 - val_loss: 34118.5664\n",
      "Epoch 998/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 17764.9023 - val_loss: 34214.9922\n",
      "Epoch 999/1000\n",
      "114/114 [==============================] - 1s 5ms/step - loss: 18030.1973 - val_loss: 34545.9375\n",
      "Epoch 1000/1000\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 17466.4336 - val_loss: 34931.0703\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 50, kernel_initializer = 'he_uniform',activation='relu',input_dim = 174))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 25, kernel_initializer = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 50, kernel_initializer = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[136018.2 ],\n",
       "       [136246.95],\n",
       "       [203184.47],\n",
       "       ...,\n",
       "       [167184.2 ],\n",
       "       [126180.84],\n",
       "       [218625.12]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred=classifier.predict(df_Test)\n",
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129051.695, 150099.22 , 196159.06 , ..., 169300.45 , 108080.62 ,\n",
       "       233303.56 ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred=model.predict(df_Test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred=pd.DataFrame(y_pred)\n",
    "# sub_df=pd.read_csv('sample_submission.csv')\n",
    "# datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "# datasets.columns=['Id','SalePrice']\n",
    "# datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booster=['gbtree','gblinear']\n",
    "# base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyper Parameter Optimization\n",
    "# n_estimators = [100, 500, 900, 1100, 1500]\n",
    "# max_depth = [2, 3, 5, 10, 15]\n",
    "# booster=['gbtree','gblinear']\n",
    "# learning_rate=[0.05,0.1,0.15,0.20]\n",
    "# min_child_weight=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the grid of hyperparameters to search\n",
    "# hyperparameter_grid = {\n",
    "#     'n_estimators': n_estimators,\n",
    "#     'max_depth':max_depth,\n",
    "#     'learning_rate':learning_rate,\n",
    "#     'min_child_weight':min_child_weight,\n",
    "#     'booster':booster,\n",
    "#     'base_score':base_score\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the random search with 4-fold cross validation\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "#             param_distributions=hyperparameter_grid,\n",
    "#             cv=5, n_iter=50,\n",
    "#             scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "#             verbose = 5, \n",
    "#             return_train_score = True,\n",
    "#             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'finalized_model.pkl'\n",
    "# pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=regressor.predict(df_Test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred=pd.DataFrame(y_pred)\n",
    "# sub_df=pd.read_csv('sample_submission.csv')\n",
    "# datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "# datasets.columns=['Id','SalePrice']\n",
    "# datasets.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
